{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "This notebook downloads metadata used in the coverage notebooks.\n",
    "\n",
    "1. Run top cell\n",
    "2. Navigate to product of interest\n",
    "3. Ensure credentials are set in text file for the corresponding API. An example credentials file is provided (credentials.txt). This should be placed in a 'credentials' folder.\n",
    "4. Configures settings in the corresponding section\n",
    "5. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import date\n",
    "import shapely.wkt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import geojson\n",
    "import time\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# metadata API's\n",
    "from sentinelsat import SentinelAPI\n",
    "from landsatxplore.api import API as landsatAPI\n",
    "import cartopy.crs as ccrs\n",
    "from cmr import CollectionQuery, GranuleQuery, ToolQuery, ServiceQuery, VariableQuery\n",
    "import earthaccess\n",
    "from eodms_rapi import EODMSRAPI\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_folder = 'metadata' # where to save metadata\n",
    "credentials_folder = 'credentials' # where to look for corresponding credential file\n",
    "\n",
    "# create folders\n",
    "fs = [metadata_folder, credentials_folder]\n",
    "for f in fs:\n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(filepath, results):\n",
    "    \"\"\"save metadata in the download loops\n",
    "\n",
    "    Args:\n",
    "        filepath (str): fill path of file\n",
    "        results (_type_): dictionary of results indexed by an id that \n",
    "        can be used to exclude duplicates\n",
    "    \n",
    "    Returns\n",
    "        duplicate_count (int), added_count (int)\n",
    "    \"\"\"\n",
    "    duplicate_count, added_count = 0, 0\n",
    "    print(f'saving data to file {filepath}')\n",
    "    if not os.path.exists(filepath):\n",
    "        with open(filepath, \"w\") as fp:\n",
    "            fp.write(json.dumps(results, indent=4, sort_keys=True, default=str))\n",
    "        added_count = len(results)\n",
    "        print(f'file created, {added_count} products added')\n",
    "    else:\n",
    "        with open(filepath, 'r') as f:\n",
    "            saved_data = json.load(f)\n",
    "        for k in results.keys():\n",
    "            if k not in saved_data:\n",
    "                saved_data[k] = results[k]\n",
    "                added_count+=1\n",
    "            else: \n",
    "                duplicate_count += 1\n",
    "        print('updating metadata file...')\n",
    "        print(f'{added_count} products added, {duplicate_count} already exist and are ignored')\n",
    "        with open(filepath, \"w\") as fp:\n",
    "            fp.write(json.dumps(saved_data, indent=4, sort_keys=True, default=str))\n",
    "    return duplicate_count, added_count\n",
    "\n",
    "def combine_subfolders(subfolders, outpath):\n",
    "    process = True\n",
    "    if process:\n",
    "        for i,sf in enumerate(subfolders):\n",
    "            files = os.listdir(f'{sf}')\n",
    "            results = {}\n",
    "            print(sf)\n",
    "            for file in tqdm(files):\n",
    "                file_name = f'{sf}/{file}'\n",
    "                with open(file_name, 'r') as f:\n",
    "                    saved_data = json.load(f)\n",
    "                for k in saved_data.keys():\n",
    "                    if k not in results:\n",
    "                        results[k] = saved_data[k]\n",
    "            \n",
    "            # naming for viirs and modis\n",
    "            # sat, prod, start, end, nth, ext = file.split('_')\n",
    "            #all_path = folder + '/' + '_'.join([sat,prod,nth,ext])\n",
    "            \n",
    "            with open(outpath, \"w\") as fp:\n",
    "                fp.write(json.dumps(results, indent=4, sort_keys=True, default=str))\n",
    "            print(outpath)\n",
    "            \n",
    "def search_cmr_data(filters, min_granules=0):\n",
    "    \n",
    "    # search the common metadata repository for AMSR level 1 data\n",
    "    df = pd.read_xml('cmr_provider_holdings.xml')\n",
    "    # check all conditions are met - see if the filter values in entry title\n",
    "    df_filt = np.array([df['entry-title'].str.upper().str.contains(x) for x in filters])\n",
    "    df_filt = np.prod(df_filt, axis=0)\n",
    "    df_filt = [bool(x) for x in df_filt] # to bools\n",
    "    data = df[df_filt]\n",
    "        \n",
    "    # get the provider name so we can search for the shortnames needed for downloading\n",
    "    providers = data['provider-id'].unique()\n",
    "    # Short names not provided here, so query a provider to get the shortnames for the granules\n",
    "    dfs = []\n",
    "    for provider in providers:\n",
    "        capi = CollectionQuery()\n",
    "        collections = capi.provider(provider) #.keyword(\"AST_L1*\").get(5)\n",
    "        provider_df = collections.get()\n",
    "        provider_df = pd.DataFrame(provider_df)\n",
    "        if 'dataset_id' not in list(provider_df):\n",
    "            continue\n",
    "        df_filt = np.array([provider_df['dataset_id'].str.upper().str.contains(x) for x in filters])\n",
    "        df_filt = np.prod(df_filt, axis=0)\n",
    "        df_filt = [bool(x) for x in df_filt] # to bools\n",
    "        data = provider_df[df_filt]\n",
    "        dfs.append(data)\n",
    "    data = pd.concat(dfs)\n",
    "    # filter for the datasets which have granules for analysis \n",
    "    has_granules = {}\n",
    "    for i, s in enumerate(data.short_name.values):\n",
    "        api = GranuleQuery()\n",
    "        granules = api.short_name(s)\n",
    "        n_found = granules.hits()\n",
    "        if n_found >= min_granules:\n",
    "            has_granules[s] = n_found\n",
    "    # filter for where we have granules\n",
    "    datasets = data[data['short_name'].isin(has_granules)]\n",
    "    datasets['granules'] = datasets['short_name'].map(has_granules)\n",
    "    return datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open(f\"{credentials_folder}/credentials_cophub.txt\", \"r\") as f:\n",
    "   txt = str(f.read())\n",
    "   user = txt.split('\\n')[1].split('login')[-1][1:]\n",
    "   password = txt.split('\\n')[2].split('password')[-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = SentinelAPI(user, password)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satellite and Product Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentinel 3\n",
    "platformname = \"Sentinel-3\"\n",
    "producttype = None #'OLCI'\n",
    "instrumentshortname =  \"OLCI\"\n",
    "\n",
    "# sentinel 2\n",
    "# platformname = \"Sentinel-2\"\n",
    "# #producttype = 'S2MSI1C'\n",
    "# producttype = 'S2MSI2Ap'\n",
    "#instrumentshortname =  None\n",
    "\n",
    "# # sentinel 1\n",
    "# platformname = \"Sentinel-1\"\n",
    "# producttype = 'GRD'#'SLC'#'GRD'\n",
    "#instrumentshortname =  None\n",
    "\n",
    "# dates\n",
    "start_date=date(2017, 10, 1)\n",
    "end_date=date(2018, 8, 1)\n",
    "chunk_days = 10 #set to a value if split the search into parts else False\n",
    "save_every = 3\n",
    "\n",
    "#northing\n",
    "bounds = east, west, south, north = -180, 180, -90, -50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESA using sentinelsat Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint = f\"POLYGON (({east} {north}, {east} {south}, {west} {south}, {west} {north}, {east} {north}))\"\n",
    "envelope = f\"ENVELOPE({east}, {west}, {north}, {south})\"\n",
    "\n",
    "# plot the search footprint\n",
    "plt.rcParams[\"figure.figsize\"] = [10,8]\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(), title='Search Area')\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "ax.add_geometries(shapely.wkt.loads(footprint), crs=ccrs.PlateCarree(), alpha=0.5)\n",
    "\n",
    "import ast\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import mapping\n",
    "import geojson\n",
    "\n",
    "geojson_string = geojson.dumps(mapping(loads(footprint)))\n",
    "geojson_dict = ast.literal_eval(geojson_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'searching for {platformname} {producttype} from {start_date} to {end_date}')\n",
    "count = api.count(envelope, \n",
    "                  date=(start_date,end_date), \n",
    "                  producttype=producttype, \n",
    "                  platformname=platformname,\n",
    "                  instrumentshortname=instrumentshortname)\n",
    "print(f'{count} products found')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_metadata = True\n",
    "save_results = True\n",
    "if download_metadata:\n",
    "    results = {}\n",
    "    if not chunk_days:\n",
    "        # search the daterange as is\n",
    "        results = api.query(envelope, \n",
    "                            date=(start_date,end_date), \n",
    "                            producttype=producttype, \n",
    "                            platformname=platformname,\n",
    "                            instrumentshortname=instrumentshortname)\n",
    "    else:\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        for d in range(0,len(dates)-1):\n",
    "            print(f'downloding chunk {d+1} of {len(dates)} ({dates[d]} to {dates[d+1]})')\n",
    "            results_chunk = api.query(envelope, \n",
    "                                      date=(dates[d],dates[d+1]), \n",
    "                                      producttype=producttype, \n",
    "                                      platformname=platformname,\n",
    "                                      instrumentshortname=instrumentshortname)\n",
    "            for k in results_chunk.keys():\n",
    "                if k not in results:\n",
    "                    results [k] = results_chunk[k]\n",
    "\n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                prod_id = producttype if producttype != None else instrumentshortname\n",
    "                filepath = os.path.join(metadata_folder,f'{platformname}_{prod_id}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landsat\n",
    "\n",
    "| Dataset Name | Dataset ID |\n",
    "|---|---|\n",
    "| Landsat 5 TM Collection 2 Level 1 | landsat_tm_c2_l1 |\n",
    "| Landsat 5 TM Collection 2 Level 2 | landsat_tm_c2_l2 |\n",
    "| Landsat 7 ETM+ Collection 2 Level 1 | landsat_etm_c2_l1 |\n",
    "| Landsat 7 ETM+ Collection 2 Level 2 | landsat_etm_c2_l2 |\n",
    "| Landsat 8 Collection 2 Level 1 | landsat_ot_c2_l1 |\n",
    "| Landsat 8 Collection 2 Level 2 | landsat_ot_c2_l2 |\n",
    "| Landsat 9 Collection 2 Level 1 | landsat_ot_c2_l1 |\n",
    "| Landsat 9 Collection 2 Level 2 | landsat_ot_c2_l2 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open(\"credentials/credentials_usgs_eros.txt\", \"r\") as f:\n",
    "   txt = str(f.read())\n",
    "   uid = txt.split('\\n')[1].split('login')[-1][1:]\n",
    "   pswd = txt.split('\\n')[2].split('password')[-1][1:]\n",
    "   email = txt.split('\\n')[3].split('email')[-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new API instance\n",
    "api = landsatAPI(uid, pswd)\n",
    "\n",
    "# Perform a check request\n",
    "response = api.request(endpoint=\"dataset-catalogs\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date='2022-01-01'\n",
    "#end_date='2022-12-31'\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "max_results=50_000\n",
    "start_date=date(2020, 1, 1)\n",
    "end_date=date(2023, 12, 31)\n",
    "chunk_days = 10 #set to a value if split the search into parts else False\n",
    "# dataset = 'landsat_etm_c2_l1' # lsat 7\n",
    "dataset = 'landsat_ot_c2_l2' # lsat 8/9\n",
    "save_every = 3\n",
    "# dataset = 'landsat_tm_c2_l2' #lsat 5\n",
    "max_results = 50_000 #50000 is the max, use chunk days to break up downloads\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_metadata = True\n",
    "save_results = True\n",
    "if download_metadata:\n",
    "    results = {}\n",
    "    if not chunk_days:\n",
    "        # search the daterange as is\n",
    "        results = api.search(\n",
    "            dataset=dataset,\n",
    "            bbox=bbox,\n",
    "            start_date=start_date.date(),\n",
    "            end_date=end_date.date(),\n",
    "            max_results=max_results,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # search chunks of data\n",
    "        new_added = 0\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date+datetime.timedelta(days=chunk_days), freq=f'{chunk_days}D'))\n",
    "        #dates.append(end_date)\n",
    "        for d in range(0,len(dates)-1):\n",
    "            print(f'downloding chunk {d+1} of {len(dates)} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            time.sleep(5) # sleep for 5 seconds, stop timeout\n",
    "            results_chunk = api.search(\n",
    "                dataset=dataset,\n",
    "                bbox=bbox,\n",
    "                start_date=str(dates[d].date()),\n",
    "                end_date=str(dates[d+1].date()),\n",
    "                max_results=max_results,\n",
    "            )\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['landsat_product_id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "\n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder, f'{dataset}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample download\n",
    "# from landsatxplore.earthexplorer import EarthExplorer\n",
    "# ee = EarthExplorer(uid, pswd)\n",
    "# #ee.download('LC08_L1GT_008113_20221123_20221205_02_T2', output_dir='data')\n",
    "# #ee.download('LC08_L1GT_041112_20180204_20201016_02_T2', output_dir='data')\n",
    "# #ee.download('LC08_L2SR_041112_20180204_20201016_02_T2', output_dir='data')\n",
    "# #LC08_L2SR_058118_20191028_20201016_02_T2\n",
    "# #LC08_L1GT_058118_20191028_20201016_02_T2\n",
    "# ee.download('LC08_L2SR_058118_20191028_20201016_02_T2', output_dir='data')\n",
    "# ee.logout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS and VIIRS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(2018, 1, 1)\n",
    "end_date=date(2021, 1, 1)\n",
    "chunk_days = 30\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_sats = ['AQUA']#,'TERRA']\n",
    "modis_list = [\n",
    "# '01', # Level 1A Scans of raw radiances in counts\t\t\n",
    "# '02QKM', # Level 1B Calibrated Radiances - 250m\t\t\n",
    "# '02HKM', # Level 1B Calibrated Radiances - 500m\t\t\n",
    "# '021KM', # Level 1B Calibrated Radiances - 1km\t(2010-07-02 start AQUA)\n",
    "# '02SSH', # Level 1B Subsampled Calibrated Radiances 5km\t\t\n",
    "# '02OBC', # Level 1B Onboard Calibrator/Engineering Data\t\t\n",
    "# '03' # Geolocation - 1km\t\t\n",
    "'09' # L2 Surface Reflectance, 5-Min Swath 250m, 500m, and 1km\n",
    " ] # Moderate Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 750m Light\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_sats = ['JPSS1'] #SUOMI_NPP',\n",
    "viirs_list = [\n",
    "#  '02IMG', # Imagery Resolution 6-Min L1B Swath 375m\t\t\n",
    "#  '02MOD', # Moderate Resolution 6-Min L1B Swath 750m\t\t\n",
    "#  '02DNB', # Day/Night Band 6-Min L1B Swath 750m\t\t\n",
    "#  '03IMG', # Imagery Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 375m\t\t\n",
    "#  '03MOD', # Moderate Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 750m\t\t\n",
    "#  '03DNB', # Day/Night Band Moderate Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 750m\t\t\n",
    "#  '03IMG', # Imagery Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 375m Light\t\t\n",
    "#  '03MOD' # Moderate Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 750m Light\t\n",
    "  '09GA' # Atmospherically Corrected Surface Reflectance 6-Min L2 Swath IP 375m, 750m NRT\n",
    " ] # Moderate Resolution Terrain-Corrected Geolocation 6-Min L1 Swath 750m Light\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS\n",
    "# plist = modis_list\n",
    "# satlist = modis_sats\n",
    "\n",
    "# VIIRS\n",
    "plist = viirs_list\n",
    "satlist = viirs_sats\n",
    "\n",
    "for collection_short_name in plist:\n",
    "    for sat in satlist:\n",
    "        \n",
    "        # MODIS\n",
    "        if sat in ['TERRA', 'AQUA']:\n",
    "            dataset = 'MOD' + collection_short_name if sat == 'TERRA' else collection_short_name\n",
    "            dataset = 'MYD' + collection_short_name if sat == 'AQUA' else dataset\n",
    "        # VIIRS\n",
    "        if sat in ['SUOMI_NPP', 'JPSS1']:\n",
    "            dataset = 'VNP' + collection_short_name if sat == 'SUOMI_NPP' else collection_short_name\n",
    "            dataset = 'VJ1' + collection_short_name if sat == 'JPSS1' else dataset\n",
    "\n",
    "        print(f'Searching {sat} {dataset} between {start_date} and {end_date}')\n",
    "        if download_metadata:\n",
    "            new_added = 0\n",
    "            # search chunks of data\n",
    "            dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "            dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "            results = {}\n",
    "            for d in range(0,len(dates)-1):\n",
    "                api = GranuleQuery()\n",
    "                print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "                granules = (api.short_name(dataset)\n",
    "                            .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                            .bounding_box(*bbox)\n",
    "                            #.point(179.9, 85)\n",
    "                )\n",
    "                n_found = granules.hits()\n",
    "                print(f'{n_found} granules found')\n",
    "                results_chunk = granules.get(n_found)\n",
    "                print(f'number of products downloaded: {len(results_chunk)}')\n",
    "                for r in results_chunk:\n",
    "                    id_ = r['id']\n",
    "                    if id_ not in results:\n",
    "                        results[id_] = r\n",
    "                \n",
    "                if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                    filepath = os.path.join(metadata_folder,f'{sat}_{dataset}_{north}N_products.json')\n",
    "                    save_metadata(filepath, results)\n",
    "                    results = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceSat-1 and IceSat-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earth Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open(\"credentials/credentials_earthaccess.txt\", \"r\") as f:\n",
    "   txt = str(f.read())\n",
    "   uid = txt.split('\\n')[1].split('login')[-1][1:]\n",
    "   pswd = txt.split('\\n')[2].split('password')[-1][1:]\n",
    "   email = txt.split('\\n')[3].split('email')[-1][1:]\n",
    "\n",
    "# set env variables for earthacces\n",
    "os.environ['EARTHDATA_USERNAME'] = uid\n",
    "os.environ['EARTHDATA_PASSWORD'] = pswd\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version(short_name):\n",
    "    # Get json response from CMR collection metadata\n",
    "    # use to search for data \n",
    "    params = {'short_name': short_name}\n",
    "    cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "    response = requests.get(cmr_collections_url, params=params)\n",
    "    results = json.loads(response.content)\n",
    "    # Find all instances of 'version_id' in metadata and print most recent version number\n",
    "    versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "    latest_version = max(versions)\n",
    "    print('The most recent version of ', short_name, ' is ', latest_version)\n",
    "    return latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=date(2018, 1, 1) \n",
    "end_date=date(2024, 1, 1)\n",
    "chunk_days = 50\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 5\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IceSat-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icesat1_list = [\n",
    "# 'GLAH01', # GLAS/ICESat L1A Global Altimetry Data (HDF5), Version 33 (GLAH01)\n",
    "# 'GLAH02', # GLAS/ICESat L1A Global Atmosphere Data (HDF5), Version 33 (GLAH02)\n",
    "# 'GLAH03', # GLAS/ICESat L1A Global Engineering Data (HDF5), Version 33 (GLAH03)\n",
    "# 'GLAH04', # GLAS/ICESat L1A Global Laser Pointing Data (HDF5), Version 33 (GLAH04)\n",
    "# 'GLAH05', # GLAS/ICESat L1B Global Waveform-based Range Corrections Data (HDF5), Version 34 (GLAH05)\n",
    "# 'GLAH06', # GLAS/ICESat L1B Global Elevation Data (HDF5), Version 34 (GLAH06)\n",
    "# 'GLAH07', # GLAS/ICESat L1B Global Backscatter Data (HDF5), Version 33 (GLAH07)\n",
    "#'GLAH08', # AEROSOL PARTICLE PROPERTIES PLANETARY BOUNDARY LAYER HEIGHT\n",
    "#'GLAH09', #\tCLOUD HEIGHT CLOUD VERTICAL DISTRIBUTION\n",
    "#'GLAH10', #\tAEROSOL BACKSCATTER AEROSOL EXTINCTION CLOUD REFLECTANCE TRANSMITTANCE\n",
    "'GLAH11', #\tAEROSOL OPTICAL DEPTH/THICKNESS CLOUD OPTICAL DEPTH/THICKNESS\n",
    "'GLAH12', #\tGLACIER ELEVATION/ICE SHEET ELEVATION GLACIER TOPOGRAPHY/ICE SHEET TOPOGRAPHY ICE SHEETS REFLECTANCE\n",
    "'GLAH13', #\tICE ROUGHNESS REFLECTANCE SEA ICE ELEVATION\n",
    "'GLAH14', #\tREFLECTANCE TERRAIN ELEVATION\n",
    "'GLAH15', #\tREFLECTANCE SEA SURFACE HEIGHT SEA SURFACE SLOPE\n",
    "]\n",
    "sat = 'IceSat1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IceSat-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://read-icesat-2.readthedocs.io/en/latest/getting_started/ICESat-2-Data-Products.html \n",
    "\n",
    "icesat2_list = [\n",
    "#'ATL01' # Raw ATLAS/ICESat-2 data are decompressed, ordered in time, and reformatted to HDF5. Lowest level accessable\n",
    "#'ATL02', # ATLAS/ICESat-2 L1B Converted Telemetry Data, Version 6 (ATL02)\n",
    "#'ATL03', # Global Geolocated Photon Data (L2) \n",
    "'ATL04' # Normalized Relative Backscatter Profiles (L2)\n",
    "]\n",
    "sat = 'IceSat2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for short_name in icesat1_list:\n",
    "    dataset = short_name\n",
    "    print(f'Searching {sat} {dataset} between {start_date} and {end_date}')\n",
    "    version = get_latest_version(short_name)\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            time.sleep(5)\n",
    "            results_chunk = earthaccess.search_data(\n",
    "                short_name=short_name,\n",
    "                version=version,\n",
    "                cloud_hosted=True,\n",
    "                bounding_box=bbox,\n",
    "                temporal=(str(dates[d].date()),str(dates[d+1].date())),\n",
    "            )\n",
    "            for r in results_chunk:\n",
    "                id_ = r.uuid\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r['umm']\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                    filepath = os.path.join(metadata_folder,f'{sat}_{dataset}_{north}N_products.json')\n",
    "                    save_metadata(filepath, results)\n",
    "                    results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "shortnames = ['ATL04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in shortnames:\n",
    "    print(f'Searching {dataset} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .short_name(dataset)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{sat}_{dataset}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot\n",
    "\n",
    "- Use the thia_downloader tool (https://github.com/olivierhagolle/theia_download/tree/master)\n",
    "- Command line tool, we generate and pass the arguments here\n",
    "- NOTE I added an extra command line param called metadata_file so I could specify where the metadata is being saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THEIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period : 1986 â€“ 2010\n",
    "start_date=date(1980, 1, 1)\n",
    "end_date=date(2010, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/SPOTWORLDHERITAGE'\n",
    "\n",
    "# make a subfolder to save page by page and later combine\n",
    "swh_folder = os.path.join(metadata_folder,'SWH')\n",
    "if not os.path.exists(swh_folder):\n",
    "    os.makedirs(swh_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = [\n",
    "# \"python theia_download/theia_download.py\",\n",
    "# #\"-c SPOTWORLDHERITAGE\",\n",
    "# \"-c SWH1\",\n",
    "# #f'-p SPOT5',\n",
    "# f\"-d {start_date}\",\n",
    "# f\"-f {end_date}\",\n",
    "# f\"--latmin {south}\",\n",
    "# f\"--latmax {north}\",\n",
    "# f\"--lonmin {east}\",\n",
    "# f\"--lonmax {west}\",\n",
    "# \"-n\",\n",
    "# \"-a theia_download/config_theia.cfg\",\n",
    "# f\"--metadata_file {folder}/spot.json\"\n",
    "# ]\n",
    "\n",
    "# import os\n",
    "# command_str = \" \".join(args)\n",
    "# print(command_str)\n",
    "# os.system(command_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "bounding_box = [-180, -90, 180, -50]\n",
    "bbox_str = \",\".join(map(str, bounding_box))\n",
    "\n",
    "total_pages = 1\n",
    "page = 0\n",
    "\n",
    "while page <= total_pages:\n",
    "    print(f'Getting results from page {page} of {total_pages}')\n",
    "    r = requests.get(\n",
    "        \"https://regards.cnes.fr/api/v1/rs-access-project/dataobjects/search?\" \\\n",
    "        f\"page={page}&\" \\\n",
    "        \"size=1000&\" \\\n",
    "        \"q=last:true AND tags:(\\\"URN:AIP:DATASET:swh:4e26c9fc-2d94-4c12-ad18-74a758e3e2ea:V1\\\" OR \\\"URN:AIP:DATASET:swh:4ddf18ab-9a49-4f59-aa23-7a8ec8fdf824:V1\\\") AND properties.DataDate:[1980-09-01T00:00:00.000Z TO 2016-07-26T00:00:00.000Z]&\" \\\n",
    "        \"g=POLYGON((-180 -90,180 -90,180 -50,-180 -50,-180 -90))&\" \\\n",
    "        \"facets=properties.PlatformName&\" \\\n",
    "        \"facets=properties.InstrumentName&\" \\\n",
    "        \"facets=properties.SensorCode&\" \\\n",
    "        \"facets=properties.Station&\" \\\n",
    "        \"facets=properties.CoupledMode&\" \\\n",
    "        \"facets=properties.CouplingModes&\" \\\n",
    "        \"sort=properties.DataID,ASC\",\n",
    "        #headers={\"Authorization\": \"Bearer {access_token}\".format(access_token=access_token)}\n",
    "        headers={\"Scope\": \"swh\"}\n",
    "        )\n",
    "    data = r.json()\n",
    "    results = data['content']\n",
    "    file_name = os.path.join(swh_folder,f'SPOT_page_{page}.json')\n",
    "    #add key to results\n",
    "    results = {r['content']['properties']['SceneName'] : r for r in results}\n",
    "    print(f\"saving {len(results)} results to {file_name}\")\n",
    "    with open(file_name, \"w\") as fp:\n",
    "        fp.write(json.dumps(results, indent=4, sort_keys=True, default=str))\n",
    "    total_pages = int(data['metadata']['totalPages'])\n",
    "    page += 1\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the subfiles\n",
    "combine_subfolders([swh_folder], f'{metadata_folder}/SPOT_L1A.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the common metadata repository metadata for AMSR level 1 data\n",
    "datasets = search_cmr_data(filters=[\n",
    "    'AMSR',\n",
    "    'L1|LEVEL 1|LEVEL-1|LEVEL1'\n",
    "], min_granules=1)\n",
    "datasets[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = search_cmr_data(filters=[\n",
    "    'AMSR',\n",
    "    'L2A|L2B'\n",
    "    #'L2|LEVEL 2|LEVEL-2|LEVEL2'\n",
    "], min_granules=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMSR_L2 = datasets[((datasets['short_name'].str.contains('Ocean|Rain|Land|L2A'))\n",
    "                   & (~datasets['short_name'].str.contains('NRT')))][['dataset_id','short_name','granules']]\n",
    "AMSR_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(1999, 1, 1) #start at 2018-01\n",
    "end_date=date(2024, 1, 1)\n",
    "chunk_days = 150\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2 = True\n",
    "#for short_name in datasets.short_name.values:\n",
    "for short_name in AMSR_L2.short_name.values:\n",
    "    print(f'Searching {short_name} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .short_name(short_name)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "                        #.point(179.9, 85)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                str_ = 'L2' if L2 else ''\n",
    "                filepath = os.path.join(metadata_folder,f'AMSR_{str_}_{short_name}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RADARSAT-1\n",
    "\n",
    "- https://py-eodms-rapi.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the common metadata repository metadata for AMSR level 1 data\n",
    "datasets = search_cmr_data(filters=[\n",
    "    'RADARSAT',\n",
    "    'L1|LEVEL 1|LEVEL-1|LEVEL1'\n",
    "], min_granules=1)\n",
    "datasets[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(1994, 1, 1)\n",
    "end_date=date(2016, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry_title in ['RADARSAT-1_LEVEL1']:\n",
    "    print(f'Searching {entry_title} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .entry_title(entry_title)\n",
    "                        #.entry_title(entry_title)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{entry_title}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRCAN\n",
    "\n",
    "- https://www.eodms-sgdot.nrcan-rncan.gc.ca/index-en.html\n",
    "- https://github.com/eodms-sgdot/py-eodms-rapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open(\"credentials/credentials_nrcan.txt\", \"r\") as f:\n",
    "   txt = str(f.read())\n",
    "   uid = txt.split('\\n')[1].split('login')[-1][1:]\n",
    "   pswd = txt.split('\\n')[2].split('password')[-1][1:]\n",
    "   email = txt.split('\\n')[3].split('email')[-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eodms_rapi import EODMSRAPI\n",
    "# Create the EODMSRAPI object\n",
    "rapi = EODMSRAPI(uid, pswd)\n",
    "# get a list of collections\n",
    "print(rapi.get_collections(as_list=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Radarsat2RawProducts'] #'Radarsat1', 'Radarsat1RawProducts', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "start_date=date(2007, 1, 1)\n",
    "end_date=date(2023, 1, 1)\n",
    "chunk_days = 50\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "\n",
    "# Create the EODMSRAPI object\n",
    "rapi = EODMSRAPI(uid, pswd)\n",
    "# Add a point to the search\n",
    "#feat = [('intersects', \"POINT (-96.47 62.4)\")]\n",
    "feat = [\n",
    "       ('intersects', [\n",
    "           (-180, -50.00),\n",
    "           (180, -50.00),\n",
    "           (180, -90.00),\n",
    "           (-180, -90.00),\n",
    "           (-180, -50.00)\n",
    "       ]\n",
    "   )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(f'Searching {dataset} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            # convert dates to format for api\n",
    "            s = str(dates[d].date()).replace('-','') + '_000000'\n",
    "            e = str(dates[d+1].date()).replace('-','') + '_000000'\n",
    "            # Set a date range for the search\n",
    "            date_range = [{\"start\": s, \"end\": e}]\n",
    "            # Submit the search to the EODMSRAPI, specifying the Collection\n",
    "            rapi.search(dataset, features=feat, dates=date_range)\n",
    "            # Get the results from the search\n",
    "            results_chunk = rapi.get_results('full')\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['recordId']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                # save the results to a product file\n",
    "                # dont save duplicates\n",
    "                dup = 0\n",
    "                file_name = folder + f'{dataset}_{north}N_products.json'\n",
    "                if not os.path.exists(file_name):\n",
    "                    with open(file_name, \"w\") as fp:\n",
    "                        fp.write(json.dumps({}, indent=4, sort_keys=True, default=str))\n",
    "                #else:\n",
    "                with open(file_name, 'r') as f:\n",
    "                    saved_data = json.load(f)\n",
    "                for k in results.keys():\n",
    "                    if k not in saved_data:\n",
    "                        saved_data[k] = results[k]\n",
    "                        new_added+=1\n",
    "                    else: \n",
    "                        dup += 1\n",
    "                print(f'{dup} already exist and are ignored')\n",
    "                print(f'Saving data to {file_name}')\n",
    "                with open(file_name, \"w\") as fp:\n",
    "                    fp.write(json.dumps(saved_data, indent=4, sort_keys=True, default=str))\n",
    "                results = {}\n",
    "                saved_data = None\n",
    "\n",
    "    print(f'{new_added} new products added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit an order using results\n",
    "order_res = rapi.order(res[0:1])\n",
    "\n",
    "# Specify a folder location to download the images\n",
    "dest = \"data\"\n",
    "\n",
    "# Download the images from the order\n",
    "dn_res = rapi.download(order_res, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVHRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVHRR = search_cmr_data(filters=['AVHRR'], min_granules=0)\n",
    "AVHRR[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERS\n",
    "- Products from ASF may only be Amplitude/Backscatter (https://asf.alaska.edu/data-sets/sar-data-sets/ers-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERS = search_cmr_data(filters=['ERS-1|ERS-2|ERS1|ERS2','L1|LEVEL 1|LEVEL-1|LEVEL1'], min_granules=1)\n",
    "ERS[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(1991, 1, 1)\n",
    "end_date=date(2013, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry_title in ['ERS-1_LEVEL1','ERS-2_LEVEL1']:\n",
    "    print(f'Searching {entry_title} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        #.short_name(entry_title)\n",
    "                        .entry_title(entry_title)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "        \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{entry_title}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials from file\n",
    "with open(\"credentials/credentials_esa_eo.txt\", \"r\") as f:\n",
    "   txt = str(f.read())\n",
    "   uid = txt.split('\\n')[1].split('login')[-1][1:]\n",
    "   pswd = txt.split('\\n')[2].split('password')[-1][1:]\n",
    "   email = txt.split('\\n')[3].split('email')[-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from asarapi.catalog import query\n",
    "from asarapi.download import log_in, log_out, request_download\n",
    "\n",
    "username = uid\n",
    "password = pswd\n",
    "output_dir = 'data'\n",
    "location = Point(16.84, -0.04)\n",
    "\n",
    "results = query(\n",
    "    area=location.wkt,\n",
    "    start=datetime(1999, 1, 1),\n",
    "    stop=datetime(2002, 1, 1),\n",
    "    orbit='ascending'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JERS-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(1991, 1, 1)\n",
    "end_date=date(1999, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['JERS','L1|LEVEL 1|LEVEL-1|LEVEL1'], min_granules=1)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry_title in ['JERS-1_LEVEL1']:\n",
    "    print(f'Searching {entry_title} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .entry_title(entry_title)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "        if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{entry_title}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envisat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['ENVISAT'], min_granules=1)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALOS (PALSAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['ALOS|PALSAR','C1|L1|LEVEL 1|LEVEL-1|LEVEL1'], min_granules=0)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['ALOS|PALSAR','C2|L2|LEVEL 2|LEVEL-2|LEVEL2'], min_granules=0)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(2005, 1, 1)\n",
    "end_date=date(2015, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for entry_title in ['ALOS_PALSAR_LEVEL1.5','ALOS_PALSAR_LEVEL1.1','ALOS_PALSAR_LEVEL1.0']:\n",
    "for entry_title in ['ALOS_PALSAR_LEVEL2.2']:\n",
    "    print(f'Searching {entry_title} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .entry_title(entry_title)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{entry_title}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRYOSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['CRYOSAT','C1|L1|LEVEL 1|LEVEL-1|LEVEL1'], min_granules=1)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(2009, 1, 1)\n",
    "end_date=date(2024, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for short_name in datasets.short_name.values:\n",
    "    print(f'Searching {short_name} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .short_name(short_name)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "            \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'{short_name}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=['GRACE'], min_granules=0)\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_cmr_data(filters=[\n",
    "    'SSM|SMMR',\n",
    "    'C1|L1|LEVEL 1|LEVEL-1|LEVEL1',\n",
    "    ], min_granules=1)\n",
    "\n",
    "data = data[(~data['dataset_id'].str.upper().str.contains('PRECIP_SSMI'))]\n",
    "data[['dataset_id','short_name','granules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings \n",
    "api = GranuleQuery()\n",
    "start_date=date(2000, 1, 1)\n",
    "end_date=date(2001, 1, 1)\n",
    "chunk_days = 100\n",
    "bounds = east, west, south, north = -180, 180, -90, -50\n",
    "bbox=(east,south,west,north)\n",
    "download_metadata = True\n",
    "save_results = True\n",
    "save_every = 10\n",
    "folder = 'metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates by sat to iterate through\n",
    "dates_by_gen = {\n",
    "    'NIMBUS7':[date(1978, 1, 1), date(1988, 1, 1)],\n",
    "    'F08':[date(1987, 1, 1), date(1992, 1, 1)],\n",
    "    'F10':[date(1992, 1, 1), date(1998, 1, 1)],\n",
    "    'F11':[date(1991, 1, 1), date(1996, 1, 1)],\n",
    "    'F13':[date(1995, 1, 1), date(2009, 1, 1)],\n",
    "    'F15':[date(2000, 1, 1), date(2024, 1, 1)],\n",
    "    'F16':[date(2005, 1, 1), date(2024, 1, 1)],\n",
    "    'F17':[date(2006, 1, 1), date(2024, 1, 1)],\n",
    "    'F08':[date(2010, 1, 1), date(2024, 1, 1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data.short_name.values:\n",
    "    for gen in dates_by_gen.keys():\n",
    "        if gen in dates_by_gen:\n",
    "            start_date, end_date = dates_by_gen[gen]\n",
    "            break\n",
    "    print(f'Searching {dataset} between {start_date} and {end_date}')\n",
    "    if download_metadata:\n",
    "        new_added = 0\n",
    "        # search chunks of data\n",
    "        dates = list(pd.date_range(start=start_date,end=end_date, freq=f'{chunk_days}D'))\n",
    "        dates.append(datetime.datetime(end_date.year, end_date.month, end_date.day))\n",
    "        results = {}\n",
    "        for d in range(0,len(dates)-1):\n",
    "            api = GranuleQuery()\n",
    "            print(f'downloding chunk {d+1} of {len(dates)-1} ({dates[d].date()} to {dates[d+1].date()})')\n",
    "            granules = (api\n",
    "                        .short_name(dataset)\n",
    "                        .temporal(date_from=dates[d].date(), date_to=dates[d+1].date())\n",
    "                        .bounding_box(*bbox)\n",
    "                        #.point(179.9, 85)\n",
    "            )\n",
    "            n_found = granules.hits()\n",
    "            print(f'{n_found} granules found')\n",
    "            results_chunk = granules.get(n_found)\n",
    "            print(f'number of products downloaded: {len(results_chunk)}')\n",
    "            for r in results_chunk:\n",
    "                id_ = r['id']\n",
    "                if id_ not in results:\n",
    "                    results[id_] = r\n",
    "    \n",
    "            if save_results and ((d%save_every==0) or ((d+1)==(len(dates)-1))):\n",
    "                filepath = os.path.join(metadata_folder,f'SSM_{dataset}_{north}N_products.json')\n",
    "                save_metadata(filepath, results)\n",
    "                results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
