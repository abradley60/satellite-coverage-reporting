{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import rasterio\n",
    "from shapely.strtree import STRtree\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "import math\n",
    "import matplotlib\n",
    "import shapely\n",
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, LineString\n",
    "import antimeridian\n",
    "\n",
    "import shared_functions as sf\n",
    "\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_by_year(df):\n",
    "    id_ = 'id' if 'id' in list(df) else 'Identifier' \n",
    "    tot = df.groupby(['year']).agg(\n",
    "    file_count = (id_, 'count'),\n",
    "    size_MB = ('size_MB','sum'),\n",
    "    size_GB = ('size_GB','sum'),\n",
    "    size_TB = ('size_TB','sum'),\n",
    "    )\n",
    "    tot.loc[\"Total\"] = tot.sum()\n",
    "    tot['avg_filesize'] = tot['size_MB']/tot['file_count']\n",
    "    return tot\n",
    "\n",
    "def summarise_by_month(df,year):\n",
    "    id_ = 'id' if 'id' in list(df) else 'Identifier' \n",
    "    tot = df[df['year']==year].groupby(['month']).agg(\n",
    "        file_count = (id_, 'count'),\n",
    "        size_MB = ('size_MB','sum'),\n",
    "        size_GB = ('size_GB','sum'),\n",
    "        size_TB = ('size_TB','sum'),\n",
    "    )\n",
    "    tot.loc[\"Total\"] = tot.sum()\n",
    "    return tot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'metadata/TERRA_MOD01_-50N_products.json'\n",
    "# filename = f'metadata/TERRA_MOD02QKM_-50N_products.json'\n",
    "# filename = f'metadata/TERRA_MOD02HKM_-50N_products.json'\n",
    "# filename = f'metadata/TERRA_MOD021KM_-50N_products.json'\n",
    "# filename = f'metadata/TERRA_MOD09_-50N_products.json' # level-2 \n",
    "filename = f'metadata/AQUA_MYD09_-50N_products.json' # level-2 \n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "     df = json.load(f)\n",
    "\n",
    "# engineering\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = preprocess_df(df)\n",
    "df = sf.preprocess_cmr_df(df, crs=3031, lon_first=True)\n",
    "df['sat_id'] = 'Terra'\n",
    "print(df.shape)\n",
    "print(df['time_start'].min(),df['time_start'].max())\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot_results_footprint_map(df.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = summarise_by_year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_by_month(df,2002)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Plot (Summer vs Winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_night_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {'DAY':'red','BOTH':'orange','NIGHT':'black'}\n",
    "\n",
    "#summer\n",
    "s = datetime.datetime.strptime('01-01-2005 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "e = datetime.datetime.strptime('02-01-2005 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "title = f\"MODIS Aqua Daily Acquisition in Summer ({s.date()})\"\n",
    "sf.plot_results_footprint_map(df[(df['time_start'] > s) & (df['time_start'] < e)], \n",
    "                           title=title,\n",
    "                           group='day_night_flag',\n",
    "                           group_colors=cols) # plot two days\n",
    "#winter\n",
    "s = datetime.datetime.strptime('01-06-2005 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "e = datetime.datetime.strptime('02-06-2005 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "title = f\"MODIS Aqua Daily Acquisition in Winter ({s.date()})\"\n",
    "sf.plot_results_footprint_map(df[(df['time_start'] > s) & (df['time_start'] < e)], \n",
    "                           title=title,\n",
    "                           group='day_night_flag',\n",
    "                           group_colors=cols) # plot two days\n",
    "#winter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise all Terra/Aqua Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_list = []\n",
    "\n",
    "filelist = [x for x in os.listdir('metadata') if (('AQUA' in x) or ('TERRA' in x))]\n",
    "\n",
    "for filename in tqdm(filelist):\n",
    "\n",
    "    print(filename)\n",
    "    sat, product = filename.split('_')[0],filename.split('_')[1]\n",
    "    with open('metadata/' + filename, 'r') as f:\n",
    "        df = json.load(f)\n",
    "\n",
    "    # engineering\n",
    "    df = pd.DataFrame.from_dict(df, orient='index')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    \n",
    "    print('Converting times')\n",
    "    df['time_start'] = pd.to_datetime(df['time_start'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['time_end'] = pd.to_datetime(df['time_end'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['month'] = df['time_start'].dt.month\n",
    "    df['month_name'] = df['time_start'].dt.month_name()\n",
    "    df['year'] = df['time_start'].dt.year\n",
    "    MAX_DATE = datetime.datetime.strptime('16/06/23', '%d/%m/%y')\n",
    "    df = df[df['time_start']<MAX_DATE] #filter for date\n",
    "    \n",
    "    #file size\n",
    "    print('Calculating size')\n",
    "    if 'granule_size' in df.columns:\n",
    "        df['size_MB'] = df['granule_size'].astype(float)\n",
    "        df['size_GB'] = df['size_MB'] / 1_000\n",
    "        df['size_TB'] = df['size_MB'] / 1_000_000\n",
    "\n",
    "    tot = df.groupby(['year']).agg(\n",
    "    file_count = ('id', 'count'),\n",
    "    size_MB = ('size_MB','sum'),\n",
    "    size_GB = ('size_GB','sum'),\n",
    "    size_TB = ('size_TB','sum'),\n",
    "    ).reset_index()\n",
    "    tot['product'] = product\n",
    "    tot['satellite'] = sat\n",
    "\n",
    "    df_list.append(tot)\n",
    "    df = None\n",
    "\n",
    "modis_summary = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_summary.to_csv('data/MODIS_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_summary = pd.read_csv('data/MODIS_summary.csv')\n",
    "val = 'file_count' # 'size_TB'\n",
    "val = 'size_TB'\n",
    "summary = modis_summary.pivot(index='year', columns='product',values=[val]).fillna(0)\n",
    "summary.loc[\"Total\"] = summary.sum()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean prod size\n",
    "prod_sum = modis_summary.groupby(['product'])[['size_MB','file_count']].sum().reset_index()\n",
    "prod_sum['mean_size_MB'] = prod_sum['size_MB'] / prod_sum['file_count']\n",
    "prod_sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'metadata/JPSS1_VJ102IMG_-50N_products.json'\n",
    "filename = f'metadata/SUOMINPP_VNP02IMG_-50N_products.json'\n",
    "filename = f'metadata/SUOMI_NPP_VNP09GA_-50N_products.json'\n",
    "filename = f'metadata/JPSS1_VJ109GA_-50N_products.json'\n",
    "\n",
    "product = filename.split('_')[1]\n",
    "with open(filename, 'r') as f:\n",
    "     df = json.load(f)\n",
    "\n",
    "# engineering\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "df = sf.preprocess_cmr_df(df, crs=3031, lon_first=True)\n",
    "# there are duplicates across different data_centers, keep just LPDAAC\n",
    "if 'SUOMI_NPP_VNP09GA' in filename:\n",
    "    df = df[df['data_center']=='LPDAAC_ECS']\n",
    "\n",
    "print(df.shape)\n",
    "print(df['time_start'].min(),df['time_start'].max())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_by_year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_by_month(df,2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = filename\n",
    "sf.plot_timeseries_products(df, title=title, stack_col='data_center', date_col='time_start',count_freq='7D', plot_freq='1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Plot (Summer vs Winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {'DAY':'red','BOTH':'orange','NIGHT':'black'}\n",
    "\n",
    "#summer\n",
    "s = datetime.datetime.strptime('01-01-2015 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "e = datetime.datetime.strptime('02-01-2015 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "title = f\"VIIRS Suomi NPP Daily Acquisition in Summer ({s.date()})\"\n",
    "sf.plot_results_footprint_map(df[(df['time_start'] > s) & (df['time_start'] < e)], \n",
    "                           title=title,\n",
    "                           group='day_night_flag',\n",
    "                           group_colors=cols) # plot two days\n",
    "#winter\n",
    "s = datetime.datetime.strptime('01-06-2015 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "e = datetime.datetime.strptime('02-06-2015 00:00:00', '%d-%m-%Y %H:%M:%S')\n",
    "title = f\"VIIRS Suomi NPP Daily Acquisition in Winter ({s.date()})\"\n",
    "sf.plot_results_footprint_map(df[(df['time_start'] > s) & (df['time_start'] < e)], \n",
    "                           title=title,\n",
    "                           group='day_night_flag',\n",
    "                           group_colors=cols) # plot two days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise all VIIRS Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_list = []\n",
    "\n",
    "filelist = [x for x in os.listdir('metadata') if (('JPSS1' in x) or ('SUOMINPP' in x))]\n",
    "\n",
    "for filename in tqdm(filelist):\n",
    "\n",
    "    print(filename)\n",
    "    sat, product = filename.split('_')[0],filename.split('_')[1]\n",
    "    with open('metadata/' + filename, 'r') as f:\n",
    "        df = json.load(f)\n",
    "\n",
    "    # engineering\n",
    "    df = pd.DataFrame.from_dict(df, orient='index')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    \n",
    "    print('Converting times')\n",
    "    df['time_start'] = pd.to_datetime(df['time_start'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['time_end'] = pd.to_datetime(df['time_end'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['month'] = df['time_start'].dt.month\n",
    "    df['month_name'] = df['time_start'].dt.month_name()\n",
    "    df['year'] = df['time_start'].dt.year\n",
    "    MAX_DATE = datetime.datetime.strptime('16/06/23', '%d/%m/%y')\n",
    "    df = df[df['time_start']<MAX_DATE] #filter for date\n",
    "    \n",
    "    #file size\n",
    "    print('Calculating size')\n",
    "    if 'granule_size' in df.columns:\n",
    "        df['size_MB'] = df['granule_size'].astype(float)\n",
    "        df['size_GB'] = df['size_MB'] / 1_000\n",
    "        df['size_TB'] = df['size_MB'] / 1_000_000\n",
    "\n",
    "    tot = df.groupby(['year']).agg(\n",
    "    file_count = ('id', 'count'),\n",
    "    size_MB = ('size_MB','sum'),\n",
    "    size_GB = ('size_GB','sum'),\n",
    "    size_TB = ('size_TB','sum'),\n",
    "    ).reset_index()\n",
    "    tot['product'] = product\n",
    "    tot['satellite'] = sat\n",
    "\n",
    "    df_list.append(tot)\n",
    "    df = None\n",
    "\n",
    "viirs_summary = pd.concat(df_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_summary.to_csv('data/VIIRS_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_summary = pd.read_csv('data/VIIRS_summary.csv')\n",
    "val = 'file_count' # \n",
    "val = 'size_TB'\n",
    "summary = viirs_summary.pivot(index='year', columns='product',values=[val]).fillna(0)\n",
    "summary.loc[\"Total\"] = summary.sum()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean prod size\n",
    "prod_sum = modis_summary.groupby(['product'])[['size_MB','file_count']].sum().reset_index()\n",
    "prod_sum['mean_size_MB'] = prod_sum['size_MB'] / prod_sum['file_count']\n",
    "prod_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceSat 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'metadata/IceSat1_GLAH05_-50N_products.json'\n",
    "\n",
    "product = filename.split('_')[1]\n",
    "with open(filename, 'r') as f:\n",
    "     df = json.load(f)\n",
    "\n",
    "# engineering\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "# convert nested dictionaries to columns\n",
    "nested = ['CollectionReference','SpatialExtent','TemporalExtent','DataGranule']\n",
    "for col in nested:\n",
    "     new_cols = pd.json_normalize(df[col])\n",
    "     new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "     df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "#convert secondary nestings\n",
    "nested = ['ArchiveAndDistributionInformation','Identifiers']#,'BoundingRectangles']\n",
    "for col in nested:\n",
    "     new_cols = pd.json_normalize(df[col].apply(lambda x : x[0]))\n",
    "     new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "     df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "print('Converting times')\n",
    "df['BeginningDateTime'] = pd.to_datetime(df['BeginningDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "df['EndingDateTime'] = pd.to_datetime(df['EndingDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "df['month'] = df['BeginningDateTime'].dt.month\n",
    "df['month_name'] = df['BeginningDateTime'].dt.month_name()\n",
    "df['year'] = df['BeginningDateTime'].dt.year\n",
    "MAX_DATE = datetime.datetime.strptime('16/06/23', '%d/%m/%y')\n",
    "df = df[df['BeginningDateTime']<MAX_DATE] #filter for date\n",
    "\n",
    "df['size_MB'] = df['Size'].copy()\n",
    "df['size_GB'] = df['size_MB']/1000\n",
    "df['size_TB'] = df['size_GB']/1000\n",
    "\n",
    "df['sat_id'] = 'IceSat-1'\n",
    "df1 = df.copy()\n",
    "\n",
    "print(df.shape)\n",
    "print(df['BeginningDateTime'].min(), df['BeginningDateTime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'IceSat-1 Level 1A ({product}) - Weekly Products' \n",
    "sf.plot_timeseries_products(df, title=title, stack_col='sat_id', date_col='BeginningDateTime',count_freq='7D', plot_freq='14D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise all IceSat-1 Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_list = []\n",
    "df_all = []\n",
    "\n",
    "filelist = [x for x in os.listdir('metadata') if 'IceSat1' in x]\n",
    "print(filelist)\n",
    "\n",
    "\n",
    "for filename in sorted(filelist):\n",
    "\n",
    "    print(filename)\n",
    "    sat, product = filename.split('_')[0],filename.split('_')[1]\n",
    "    with open('metadata/' + filename, 'r') as f:\n",
    "        df = json.load(f)\n",
    "\n",
    "    # engineering\n",
    "    df = pd.DataFrame.from_dict(df, orient='index')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    df['sat_id'] = 'IceSat-1'\n",
    "\n",
    "    # convert nested dictionaries to columns\n",
    "    nested = ['CollectionReference','SpatialExtent','TemporalExtent','DataGranule']\n",
    "    for col in nested:\n",
    "        new_cols = pd.json_normalize(df[col])\n",
    "        new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "        df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "    #conver secondary nestings\n",
    "    nested = ['ArchiveAndDistributionInformation','Identifiers']#,'BoundingRectangles']\n",
    "    for col in nested:\n",
    "        new_cols = pd.json_normalize(df[col].apply(lambda x : x[0]))\n",
    "        new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "        df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "    print('Converting times')\n",
    "    df['BeginningDateTime'] = pd.to_datetime(df['BeginningDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['EndingDateTime'] = pd.to_datetime(df['EndingDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    df['month'] = df['BeginningDateTime'].dt.month\n",
    "    df['month_name'] = df['BeginningDateTime'].dt.month_name()\n",
    "    df['year'] = df['BeginningDateTime'].dt.year\n",
    "    MAX_DATE = datetime.datetime.strptime('16/06/23', '%d/%m/%y')\n",
    "    df = df[df['BeginningDateTime']<MAX_DATE] #filter for date\n",
    "\n",
    "    df['size_MB'] = df['Size'].copy()\n",
    "    df['size_GB'] = df['size_MB']/1000\n",
    "    df['size_TB'] = df['size_GB']/1000\n",
    "\n",
    "    tot = df.groupby(['year']).agg(\n",
    "        start = ('BeginningDateTime', 'min'),\n",
    "        end = ('BeginningDateTime', 'max'),\n",
    "        file_count = ('Name', 'count'),\n",
    "        size_MB = ('size_MB','sum'),\n",
    "        size_GB = ('size_GB','sum'),\n",
    "        size_TB = ('size_TB','sum'),\n",
    "        ).reset_index()\n",
    "    tot['product'] = product\n",
    "    tot['satellite'] = sat\n",
    "\n",
    "    df_list.append(tot)\n",
    "    df_all.append(df)\n",
    "    df = None\n",
    "    #break\n",
    "\n",
    "icesat1_summary = pd.concat(df_list)\n",
    "df_all = pd.concat(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'file_count' # 'size_TB'\n",
    "val = 'size_TB'\n",
    "summary = icesat1_summary.pivot(index='year', columns='product',values=[val]).fillna(0)\n",
    "summary.loc[\"Total\"] = summary.sum()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean prod size\n",
    "prod_sum =  icesat1_summary.groupby('product').agg(\n",
    "    start = ('start','min'),\n",
    "    end=('end', 'max'),\n",
    "    size_MB = ('size_MB','sum'),\n",
    "    file_count = ('file_count','sum'),\n",
    ").reset_index()\n",
    "prod_sum['mean_size_MB'] = prod_sum['size_MB'] / prod_sum['file_count']\n",
    "prod_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'IceSat-1 - Weekly L2 Products' \n",
    "df_all['product'] = df_all['Identifier'].apply(lambda x : x.split('_')[0])\n",
    "sf.plot_timeseries_products(df_all, title=title, stack_col='product', date_col='BeginningDateTime',count_freq='7D', plot_freq='1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceSat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'metadata/IceSat2_ATL02_-50N_products.json'\n",
    "filename = f'metadata/IceSat2_ATL03_-50N_products.json'\n",
    "#filename = f'metadata/IceSat2_ATL04_-50N_products.json'\n",
    "\n",
    "product = filename.split('_')[1]\n",
    "with open(filename, 'r') as f:\n",
    "     df = json.load(f)\n",
    "\n",
    "# engineering\n",
    "df = pd.DataFrame.from_dict(df, orient='index')\n",
    "print(df.shape)\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sat_id'] = 'IceSat-2'\n",
    "\n",
    "# convert nested dictionaries to columns\n",
    "nested = ['CollectionReference','SpatialExtent','TemporalExtent','DataGranule']\n",
    "for col in nested:\n",
    "    new_cols = pd.json_normalize(df[col])\n",
    "    new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "    df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "#convert secondary nestings\n",
    "nested = ['ArchiveAndDistributionInformation','Identifiers',]\n",
    "nested = nested + ['BoundingRectangles'] if 'BoundingRectangles' in list(df) else nested\n",
    "for col in nested:\n",
    "    new_cols = pd.json_normalize(df[col].apply(lambda x : x[0]))\n",
    "    new_cols.columns = [x.split('.')[-1] for x in new_cols.columns]\n",
    "    df[new_cols.columns] = new_cols[new_cols.columns].values\n",
    "\n",
    "print('Converting times')\n",
    "df['BeginningDateTime'] = pd.to_datetime(df['BeginningDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "df['EndingDateTime'] = pd.to_datetime(df['EndingDateTime'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "df['month'] = df['BeginningDateTime'].dt.month\n",
    "df['month_name'] = df['BeginningDateTime'].dt.month_name()\n",
    "df['year'] = df['BeginningDateTime'].dt.year\n",
    "MAX_DATE = datetime.datetime.strptime('16/06/23', '%d/%m/%y')\n",
    "df = df[df['BeginningDateTime']<MAX_DATE] #filter for date\n",
    "\n",
    "df['size_MB'] = df['Size'].copy()\n",
    "df['size_GB'] = df['size_MB']/1000\n",
    "df['size_TB'] = df['size_GB']/1000\n",
    "df2 = df.copy()\n",
    "\n",
    "print(list(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{filename} - Weekly Products' \n",
    "sf.plot_timeseries_products(df, title=title, stack_col='sat_id', date_col='BeginningDateTime',count_freq='7D', plot_freq='1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['BeginningDateTime'].min(), df['BeginningDateTime'].max())\n",
    "summarise_by_year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_by_month(df, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICESat 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both = pd.concat([df1[['sat_id','BeginningDateTime','Size']],df2[['sat_id','BeginningDateTime','Size']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'IceSat-1 and IceSat-2 Level 1B - Weekly Products' \n",
    "sf.plot_timeseries_products(df_both.reset_index(), title=title, stack_col='sat_id', date_col='BeginningDateTime',count_freq='7D', plot_freq='1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orbit shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icesat2_ant_orbit = gpd.read_file('icesat2_antarcticaallorbits/Antarctica_repeat1_GT7.geojson')\n",
    "icesat2_ant_orbit = icesat2_ant_orbit.set_geometry('geometry').set_crs(4326)\n",
    "icesat2_ant_orbit = icesat2_ant_orbit.to_crs(3031)\n",
    "icesat2_ant_orbit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icesat2_ant_orbit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "east, west, south, north = -180, 180, -90, -50\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]\n",
    "ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "ax.set_extent((east, west, south, north+1), ccrs.PlateCarree())\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "icesat2_ant_orbit.head(1000).plot(figsize=(10,10),lw=0.2,ax=ax, color='red')\n",
    "ax.add_feature(cartopy.feature.COASTLINE)\n",
    "ax.gridlines(draw_labels=True)\n",
    "#plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icesat2_ant_orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
